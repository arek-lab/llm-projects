{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607e0823-e265-4ebd-94b8-d8c5ab0471d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib sentence-transformers chromadb tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b2c555-beee-4eed-9894-42df37a5aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import email\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.credentials import Credentials\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from chromadb.utils import embedding_functions\n",
    "import gradio as gr\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9362926b-034d-4b2c-b24a-1f940f4ed643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    \"\"\"Removes HTML tags and returns clean text.\"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_links(text):\n",
    "    \"\"\"Removes all HTTP/HTTPS links from the text.\"\"\"\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "def get_header(headers, name):\n",
    "    \"\"\"Helper function to extract a specific header (e.g., Subject, From, Date).\"\"\"\n",
    "    for header in headers:\n",
    "        if header['name'].lower() == name.lower():\n",
    "            return header['value']\n",
    "    return None\n",
    "\n",
    "def clean_email_body(text):\n",
    "    \"\"\"Removes soft hyphens, zero-width spaces, and similar invisible characters from the text.\"\"\"\n",
    "    cleaned = re.sub(\n",
    "        r'[\\u2007\\u200b\\u200d\\u200e\\u200f\\u202a-\\u202e\\u2060\\u2061\\u2062\\u2063\\u2064\\ufeff\\u00ad]', \n",
    "        '', \n",
    "        text\n",
    "    )\n",
    "    return cleaned\n",
    "\n",
    "def get_message_body(payload):\n",
    "    \"\"\"\n",
    "    Recursively extracts the email body (text part only),\n",
    "    decodes it, cleans it from invisible characters, and removes links.\n",
    "    \"\"\"\n",
    "    if 'parts' in payload:\n",
    "        for part in payload['parts']:\n",
    "            if part['mimeType'] == 'text/plain':\n",
    "                data = part['body'].get('data')\n",
    "                if data:\n",
    "                    decoded_data = base64.urlsafe_b64decode(data).decode('utf-8')\n",
    "                    decoded_data = clean_email_body(decoded_data)\n",
    "                    clean_text = remove_links(decoded_data)\n",
    "                    return clean_text\n",
    "            else:\n",
    "                # Recursively check nested parts\n",
    "                result = get_message_body(part)\n",
    "                if result:\n",
    "                    return remove_links(result)\n",
    "    else:\n",
    "        if payload.get('mimeType') == 'text/plain':\n",
    "            data = payload['body'].get('data')\n",
    "            if data:\n",
    "                decoded_data = base64.urlsafe_b64decode(data).decode('utf-8')\n",
    "                decoded_data = clean_email_body(decoded_data)\n",
    "                return remove_links(decoded_data)\n",
    "    return \"\"\n",
    "\n",
    "def store_messages_in_dict(messages, service):\n",
    "    \"\"\"\n",
    "    Fetches messages and stores them in a list of dictionaries.\n",
    "    Each dictionary contains: id, date, sender, subject, and cleaned body text.\n",
    "    \"\"\"\n",
    "    emails_data = [] \n",
    "    if not messages:\n",
    "        print('No emails found.')\n",
    "    else:\n",
    "        for msg in messages:\n",
    "            msg_data = service.users().messages().get(userId='me', id=msg['id'], format='full').execute()\n",
    "            payload = msg_data.get('payload', {})\n",
    "            headers = payload.get('headers', [])\n",
    "\n",
    "            email = {\n",
    "                'id': msg['id'],\n",
    "                'date': get_header(headers, 'Date'),\n",
    "                'from': get_header(headers, 'From'),\n",
    "                'title': get_header(headers, 'Subject'),\n",
    "                'body': clean_html(get_message_body(payload))\n",
    "            }\n",
    "\n",
    "            emails_data.append(email)\n",
    "    return emails_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650fd56b-6774-4ac4-9ce7-036bfbd2b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "token_limit = 300\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(\"emails_collection\")\n",
    "\n",
    "def chunk_text(text, max_tokens=token_limit):\n",
    "    \"\"\"Splits the text into chunks limited by the number of tokens.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def embed_and_store_email(email):\n",
    "    \"\"\"Embeds and stores a single email into ChromaDB.\"\"\"\n",
    "    doc_id = email['id']\n",
    "    metadata_base = {\n",
    "        \"email_id\": doc_id,\n",
    "        \"from\": email['from'],\n",
    "        \"date\": email['date']\n",
    "    }\n",
    "\n",
    "    # Title embedding\n",
    "    title_embedding = embedding_model.encode([email['title']])[0]\n",
    "    collection.add(\n",
    "        documents=[email['title']],\n",
    "        metadatas=[{**metadata_base, \"part\": \"title\"}],\n",
    "        ids=[f\"{doc_id}_title\"],\n",
    "        embeddings=[title_embedding.tolist()]\n",
    "    )\n",
    "\n",
    "    # Body embedding with chunking\n",
    "    body_chunks = chunk_text(email['body'])\n",
    "    for idx, chunk in enumerate(body_chunks):\n",
    "        chunk_embedding = embedding_model.encode([chunk])[0]\n",
    "        collection.add(\n",
    "            documents=[chunk],\n",
    "            metadatas=[{**metadata_base, \"part\": f\"body_chunk_{idx}\"}],\n",
    "            ids=[f\"{doc_id}_body_chunk_{idx}\"],\n",
    "            embeddings=[chunk_embedding.tolist()]\n",
    "        )\n",
    "\n",
    "def process_and_store_emails(emails):\n",
    "    \"\"\"Processes a list of emails: embedding + storing into ChromaDB.\"\"\"\n",
    "    if not emails:\n",
    "        return \"No emails to process.\"\n",
    "\n",
    "    for email in emails:\n",
    "        embed_and_store_email(email)\n",
    "\n",
    "    return f\"Stored {len(emails)} emails in ChromaDB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cf04e1-46a5-4f36-b176-5b445cf3ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails_from_gmail(maxEmails, batch_size=100):\n",
    "    creds = Credentials.from_authorized_user_file('token.json', ['https://www.googleapis.com/auth/gmail.readonly'])\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "    all_messages = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_messages) < maxEmails:\n",
    "        remaining = maxEmails - len(all_messages)\n",
    "        this_batch_size = min(batch_size, remaining)  # the last batch may be smaller than batch_size\n",
    "\n",
    "        results = service.users().messages().list(\n",
    "            userId='me',\n",
    "            maxResults=this_batch_size,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        messages = results.get('messages', [])\n",
    "        all_messages.extend(messages)\n",
    "\n",
    "        next_page_token = results.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break  # no more emails to fetch\n",
    "\n",
    "    emails_data = store_messages_in_dict(all_messages, service)\n",
    "    process_and_store_emails(emails_data)\n",
    "    return (f'Done for {len(all_messages)} records')\n",
    "\n",
    "def save_credentials(file):\n",
    "    if file is None:\n",
    "        return \"No file to save.\"\n",
    "\n",
    "    filename = file.name.split(\"/\")[-1] \n",
    "    if filename != \"credentials.json\":\n",
    "        return \"Incorrect file name. Expected: credentials.json\"\n",
    " \n",
    "    dest_path = os.path.join(os.getcwd(), \"credentials.json\")\n",
    "    shutil.copy(file.name, dest_path)\n",
    "    return f\"File saved as {dest_path}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc61293-b5b2-42ba-9781-d1bb61c44596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, num_results=5):\n",
    "    \"\"\"\n",
    "    Wyszukuje semantycznie podobne emaile w bazie ChromaDB\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not query.strip():\n",
    "            return \"Proszę wprowadzić zapytanie.\"\n",
    "        \n",
    "        # Sprawdź czy kolekcja ma jakieś dokumenty\n",
    "        collection_count = collection.count()\n",
    "        if collection_count == 0:\n",
    "            return \"Baza danych jest pusta. Najpierw pobierz emaile z Gmail.\"\n",
    "        \n",
    "        # Wykonaj wyszukiwanie semantyczne\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=min(num_results, collection_count)\n",
    "        )\n",
    "        \n",
    "        if not results['documents'] or not results['documents'][0]:\n",
    "            return \"Nie znaleziono żadnych wyników dla tego zapytania.\"\n",
    "        \n",
    "        # Formatuj wyniki\n",
    "        formatted_results = []\n",
    "        documents = results['documents'][0]\n",
    "        metadatas = results['metadatas'][0] if results['metadatas'] else [{}] * len(documents)\n",
    "        distances = results['distances'][0] if results['distances'] else [0] * len(documents)\n",
    "\n",
    "        SIMILARITY_THRESHOLD = 1.2 \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):\n",
    "            if distance > SIMILARITY_THRESHOLD:\n",
    "                if i == 0:  # jeśli nawet pierwszy wynik jest słaby\n",
    "                    return f\"Nie znaleziono podobnych emaili dla zapytania '{query}'. \\nNajlepszy wynik miał odległość {distance:.3f}, co wskazuje na słabe dopasowanie.\"\n",
    "                break  # przestań dodawać słabe wyniki\n",
    "            result_text = f\"**Wynik {i+1}** (odległość: {distance:.3f})\\n\"\n",
    "            \n",
    "            # Dodaj metadane jeśli są dostępne\n",
    "            if metadata:\n",
    "                if 'subject' in metadata:\n",
    "                    result_text += f\"**Temat:** {metadata['subject']}\\n\"\n",
    "                if 'sender' in metadata:\n",
    "                    result_text += f\"**Nadawca:** {metadata['sender']}\\n\"\n",
    "                if 'date' in metadata:\n",
    "                    result_text += f\"**Data:** {metadata['date']}\\n\"\n",
    "            \n",
    "            # Dodaj fragment treści (ograniczone do 300 znaków)\n",
    "            content_preview = doc[:300] + \"...\" if len(doc) > 300 else doc\n",
    "            result_text += f\"**Treść:** {content_preview}\\n\"\n",
    "            result_text += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            formatted_results.append(result_text)\n",
    "        \n",
    "        return \"\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Błąd podczas wyszukiwania: {str(e)}\"\n",
    "\n",
    "def get_collection_stats():\n",
    "    \"\"\"\n",
    "    Zwraca statystyki kolekcji\n",
    "    \"\"\"\n",
    "    try:\n",
    "        count = collection.count()\n",
    "        return f\"Liczba emaili w bazie: {count}\"\n",
    "    except Exception as e:\n",
    "        return f\"Błąd przy pobieraniu statystyk: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3370c6a-bf15-4d00-a433-6db320ed7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"## Load Gmail inbox and perform semantic search\")\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(elem_id=\"col2\"):\n",
    "            file_input = gr.File(label=\"Upload credentials.json\")  # file upload input for credentials\n",
    "        with gr.Column():\n",
    "            file_output = gr.Textbox(label=\"Save status\", interactive=False)  # output box to show save status\n",
    "            file_btn = gr.Button(\"Save credentials.json\")  # button to trigger saving the credentials file\n",
    "\n",
    "    \n",
    "    file_btn.click(save_credentials, inputs=[file_input], outputs=[file_output])\n",
    "\n",
    "    with gr.Row():\n",
    "        email_input = gr.Number(label=\"Number of emails to fetch\", value=10)  # input for number of emails to download\n",
    "        email_output = gr.Textbox(label=\"Download status\")  # output box to show download status\n",
    "\n",
    "    email_btn = gr.Button(\"Fetch emails\")  # button to trigger fetching emails\n",
    "    email_btn.click(get_emails_from_gmail, inputs=[email_input], outputs=[email_output])\n",
    "\n",
    "    gr.Markdown(\"### Semantic search\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            stats_output = gr.Textbox(label=\"Database statistics\", interactive=False)\n",
    "            stats_btn = gr.Button(\"Refresh statistics\")\n",
    "            stats_btn.click(get_collection_stats, outputs=[stats_output])\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            search_query = gr.Textbox(\n",
    "                label=\"Query\", \n",
    "                placeholder=\"E.g. 'meeting next week' or 'invoice for January'\",\n",
    "                lines=2\n",
    "            )\n",
    "            num_results = gr.Slider(\n",
    "                label=\"Number of results\", \n",
    "                minimum=1, \n",
    "                maximum=20, \n",
    "                value=5, \n",
    "                step=1\n",
    "            )\n",
    "            search_btn = gr.Button(\"Search\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            search_results = gr.Textbox(\n",
    "                label=\"Search results\", \n",
    "                lines=15, \n",
    "                interactive=False\n",
    "            )\n",
    "    \n",
    "    search_btn.click(\n",
    "        semantic_search, \n",
    "        inputs=[search_query, num_results], \n",
    "        outputs=[search_results]\n",
    "    )\n",
    "    \n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726570c5-0ccb-4484-a57b-7ba97ca68e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
